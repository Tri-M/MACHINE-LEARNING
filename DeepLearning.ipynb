{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMqmIjjghqrXVCFdmFNATg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tri-M/MACHINE-LEARNING/blob/master/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification with Sigmoid Activation Function - Perceptron\n"
      ],
      "metadata": {
        "id": "6jt8pLi-Bd5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PerceptronSigmoid:\n",
        "    def __init__(self, input_dim, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = np.zeros(input_dim + 1)  # +1 for the bias term\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "        return self.sigmoid(linear_output)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            predictions = self.predict(X)\n",
        "            errors = y - predictions\n",
        "            # Update weights using gradient descent\n",
        "            self.weights[1:] += self.learning_rate * np.dot(X.T, errors * predictions * (1 - predictions)) / len(y)\n",
        "            self.weights[0] += self.learning_rate * np.sum(errors * predictions * (1 - predictions)) / len(y)\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = np.array([[2, 3], [1, 5], [2, 1], [4, 4]])\n",
        "y = np.array([1, 1, 0, 0])  # Binary labels\n",
        "\n",
        "# Train perceptron with sigmoid activation\n",
        "perceptron = PerceptronSigmoid(input_dim=2)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = perceptron.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBzHdvb6Befg",
        "outputId": "ff159dac-63c9-4a82-c089-dcc0c4c020ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0.52260748 0.84023827 0.30241508 0.32459358]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression with Sigmoid Activation Function"
      ],
      "metadata": {
        "id": "q0dVu3rcBlLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, input_dim, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = np.zeros(input_dim + 1)  # +1 for the bias term\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.weights[1:]) + self.weights[0])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            predictions = self.predict(X)\n",
        "            errors = y - predictions\n",
        "            self.weights[1:] += self.learning_rate * np.dot(X.T, errors * predictions * (1 - predictions)) / len(y)\n",
        "            self.weights[0] += self.learning_rate * np.sum(errors * predictions * (1 - predictions)) / len(y)\n",
        "\n",
        "# Example dataset (features and target values)\n",
        "X = np.array([[2, 3], [1, 5], [2, 1], [4, 4]])\n",
        "y = np.array([0.7, 0.9, 0.5, 0.8])  # Continuous values between 0 and 1\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(input_dim=2)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzqBRMc0Bnse",
        "outputId": "d5392a91-eb98-4316-c1eb-5e2cd1ff1783"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0.7239555  0.82483385 0.58901449 0.7865573 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Clasification using Sigmoid"
      ],
      "metadata": {
        "id": "6Fu4bfbvE3dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PerceptronSigmoid:\n",
        "    def __init__(self, input_dim, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = np.zeros(input_dim + 1)  # +1 for the bias term\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "        return self.sigmoid(linear_output)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            predictions = self.predict(X)\n",
        "            errors = y - predictions\n",
        "            # Update weights using gradient descent\n",
        "            self.weights[1:] += self.learning_rate * np.dot(X.T, errors * predictions * (1 - predictions)) / len(y)\n",
        "            self.weights[0] += self.learning_rate * np.sum(errors * predictions * (1 - predictions)) / len(y)\n",
        "\n",
        "# Example dataset (features and binary labels)\n",
        "X = np.array([[2, 3], [1, 5], [2, 1], [4, 4]])\n",
        "y = np.array([1, 1, 0, 0])  # Binary labels\n",
        "\n",
        "# Train perceptron with sigmoid activation\n",
        "perceptron = PerceptronSigmoid(input_dim=2)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = perceptron.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytFrauNUE8jI",
        "outputId": "5999d8f3-f35f-45b9-cd7d-2d2bb1d1f1c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0.52260748 0.84023827 0.30241508 0.32459358]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiway Classification using sigmoid"
      ],
      "metadata": {
        "id": "L6JkYzeFE-VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxClassifier:\n",
        "    def __init__(self, input_dim, num_classes, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.num_classes = num_classes\n",
        "        self.weights = np.zeros((input_dim + 1, num_classes))  # +1 for the bias term\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "        return self.softmax(linear_output)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_one_hot = np.eye(self.num_classes)[y]  # Convert labels to one-hot encoding\n",
        "        for _ in range(self.epochs):\n",
        "            predictions = self.predict(X)\n",
        "            errors = y_one_hot - predictions\n",
        "            # Update weights using gradient descent\n",
        "            self.weights[1:] += self.learning_rate * np.dot(X.T, errors) / len(y)\n",
        "            self.weights[0] += self.learning_rate * np.sum(errors, axis=0) / len(y)\n",
        "\n",
        "# Example dataset (features and multi-class labels)\n",
        "X = np.array([[2, 3], [1, 5], [2, 1], [4, 4], [5, 2]])\n",
        "y = np.array([0, 1, 1, 2, 2])  # Multi-class labels (0, 1, 2)\n",
        "\n",
        "# Train softmax classifier\n",
        "num_classes = 3\n",
        "model = SoftmaxClassifier(input_dim=2, num_classes=num_classes)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions (probabilities):\", predictions)\n",
        "print(\"Predicted class labels:\", np.argmax(predictions, axis=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEkVDynZFBka",
        "outputId": "30551e42-5284-4f8c-bf78-e1639e5c3f1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions (probabilities): [[0.27858534 0.47170944 0.24970522]\n",
            " [0.25279805 0.71098736 0.03621459]\n",
            " [0.23183402 0.29949563 0.46867034]\n",
            " [0.14704844 0.17749676 0.6754548 ]\n",
            " [0.03282211 0.02385199 0.94332591]]\n",
            "Predicted class labels: [1 1 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP classification"
      ],
      "metadata": {
        "id": "zEjUVQbeFdvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n",
        "        self.b2 = np.zeros(output_dim)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, y, a2):\n",
        "        return np.mean(-y * np.log(a2 + 1e-8) - (1 - y) * np.log(1 - a2 + 1e-8))\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        m = y.shape[0]\n",
        "\n",
        "        # Reshape y to be (m, 1)\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        # Compute gradients for W2, b2\n",
        "        dz2 = self.a2 - y\n",
        "        dW2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0) / m\n",
        "\n",
        "        # Compute gradients for W1, b1\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self.sigmoid_derivative(self.a1)\n",
        "        dW1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            a2 = self.forward(X)\n",
        "            loss = self.compute_loss(y, a2)\n",
        "            self.backward(X, y)\n",
        "            if _ % 100 == 0:\n",
        "                print(f'Epoch {_}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Example dataset (features and binary labels)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "y = np.random.randint(0, 2, 100)  # Binary labels\n",
        "\n",
        "# Train MLP\n",
        "mlp = MLP(input_dim=10, hidden_dim=5, output_dim=1, learning_rate=0.01, epochs=1000)\n",
        "mlp.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = mlp.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNiVi-UaFgML",
        "outputId": "1561cb8f-27ae-4728-aa0f-41c494688328"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931783293600098\n",
            "Epoch 100, Loss: 0.6926128260121536\n",
            "Epoch 200, Loss: 0.6924314772416237\n",
            "Epoch 300, Loss: 0.6923735225274599\n",
            "Epoch 400, Loss: 0.692355128226725\n",
            "Epoch 500, Loss: 0.6923493669584837\n",
            "Epoch 600, Loss: 0.6923476087338131\n",
            "Epoch 700, Loss: 0.6923471002715745\n",
            "Epoch 800, Loss: 0.6923469708440018\n",
            "Epoch 900, Loss: 0.6923469496615973\n",
            "Predictions: [[0.4799591 ]\n",
            " [0.48000719]\n",
            " [0.47991922]\n",
            " [0.47989733]\n",
            " [0.47992911]\n",
            " [0.47989523]\n",
            " [0.47996447]\n",
            " [0.47991671]\n",
            " [0.47993646]\n",
            " [0.48000181]\n",
            " [0.47999933]\n",
            " [0.47997769]\n",
            " [0.47993663]\n",
            " [0.47995854]\n",
            " [0.47998023]\n",
            " [0.47994635]\n",
            " [0.47992962]\n",
            " [0.47999499]\n",
            " [0.47999998]\n",
            " [0.47999831]\n",
            " [0.47996931]\n",
            " [0.47996174]\n",
            " [0.47994651]\n",
            " [0.47991113]\n",
            " [0.47988388]\n",
            " [0.48000952]\n",
            " [0.47990862]\n",
            " [0.47994521]\n",
            " [0.47996503]\n",
            " [0.48002822]\n",
            " [0.47994783]\n",
            " [0.47993097]\n",
            " [0.47994877]\n",
            " [0.47998252]\n",
            " [0.47992836]\n",
            " [0.47991127]\n",
            " [0.47999852]\n",
            " [0.4799902 ]\n",
            " [0.47993428]\n",
            " [0.47988641]\n",
            " [0.47992602]\n",
            " [0.47996363]\n",
            " [0.47990122]\n",
            " [0.47994671]\n",
            " [0.47995712]\n",
            " [0.47991414]\n",
            " [0.47998722]\n",
            " [0.47987732]\n",
            " [0.47987133]\n",
            " [0.47993807]\n",
            " [0.47996094]\n",
            " [0.479998  ]\n",
            " [0.47997453]\n",
            " [0.47993463]\n",
            " [0.4798941 ]\n",
            " [0.47993036]\n",
            " [0.4799166 ]\n",
            " [0.47989691]\n",
            " [0.47995367]\n",
            " [0.479978  ]\n",
            " [0.47994824]\n",
            " [0.47992699]\n",
            " [0.47999615]\n",
            " [0.47993188]\n",
            " [0.4799412 ]\n",
            " [0.47999525]\n",
            " [0.47997253]\n",
            " [0.47986866]\n",
            " [0.47992417]\n",
            " [0.47995291]\n",
            " [0.47991082]\n",
            " [0.47991407]\n",
            " [0.47987919]\n",
            " [0.4799431 ]\n",
            " [0.47993041]\n",
            " [0.47994   ]\n",
            " [0.47996263]\n",
            " [0.47995919]\n",
            " [0.47988635]\n",
            " [0.47994565]\n",
            " [0.47996349]\n",
            " [0.47989964]\n",
            " [0.47989983]\n",
            " [0.47997277]\n",
            " [0.47994169]\n",
            " [0.47996027]\n",
            " [0.47990383]\n",
            " [0.4800196 ]\n",
            " [0.47998227]\n",
            " [0.47997834]\n",
            " [0.47995457]\n",
            " [0.47996476]\n",
            " [0.47997871]\n",
            " [0.47999174]\n",
            " [0.47998534]\n",
            " [0.4799247 ]\n",
            " [0.47997851]\n",
            " [0.47995712]\n",
            " [0.47994243]\n",
            " [0.4799536 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Regression"
      ],
      "metadata": {
        "id": "EtLsXCbYFi0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLPRegressor:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_dim, hidden_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, output_dim)\n",
        "        self.b2 = np.zeros(output_dim)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        return self.z2\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        m = y.shape[0]\n",
        "\n",
        "        # Compute gradients for W2, b2\n",
        "        dz2 = self.forward(X) - y\n",
        "        dW2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0) / m\n",
        "\n",
        "        # Compute gradients for W1, b1\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self.relu_derivative(self.z1)\n",
        "        dW1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            self.backward(X, y)\n",
        "            if _ % 100 == 0:\n",
        "                print(f'Epoch {_}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Example dataset (features and target values)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "y = np.random.rand(100, 1)   # Continuous target values\n",
        "\n",
        "# Train MLP\n",
        "mlp_reg = MLPRegressor(input_dim=10, hidden_dim=5, output_dim=1, learning_rate=0.01, epochs=1000)\n",
        "mlp_reg.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = mlp_reg.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a1d8ePNFlEx",
        "outputId": "d6076d9f-b312-4dfd-a6ea-e3b5ee072b04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 10.869375306356314\n",
            "Epoch 100, Loss: 0.18900485819725074\n",
            "Epoch 200, Loss: 0.12452007460522566\n",
            "Epoch 300, Loss: 0.11235214249940041\n",
            "Epoch 400, Loss: 0.10829604917250438\n",
            "Epoch 500, Loss: 0.1060272104536896\n",
            "Epoch 600, Loss: 0.1043683115715168\n",
            "Epoch 700, Loss: 0.10302721098598173\n",
            "Epoch 800, Loss: 0.10189833917470714\n",
            "Epoch 900, Loss: 0.10092803868688298\n",
            "Predictions: [[0.45013343]\n",
            " [0.37824011]\n",
            " [0.53866085]\n",
            " [0.68261682]\n",
            " [0.55700662]\n",
            " [0.51186834]\n",
            " [0.32081977]\n",
            " [0.7005094 ]\n",
            " [0.62238406]\n",
            " [0.46001217]\n",
            " [0.53368965]\n",
            " [0.60571267]\n",
            " [0.61916271]\n",
            " [0.47860233]\n",
            " [0.40989641]\n",
            " [0.58748714]\n",
            " [0.34937402]\n",
            " [0.49121772]\n",
            " [0.56205557]\n",
            " [0.53462785]\n",
            " [0.43219494]\n",
            " [0.52569841]\n",
            " [0.53462082]\n",
            " [0.45986811]\n",
            " [0.69829178]\n",
            " [0.50856415]\n",
            " [0.76733824]\n",
            " [0.61119015]\n",
            " [0.45175996]\n",
            " [0.39052283]\n",
            " [0.60175882]\n",
            " [0.62344811]\n",
            " [0.666314  ]\n",
            " [0.52998785]\n",
            " [0.44766833]\n",
            " [0.50789685]\n",
            " [0.68625701]\n",
            " [0.47434851]\n",
            " [0.49831631]\n",
            " [0.69457091]\n",
            " [0.55243476]\n",
            " [0.44842768]\n",
            " [0.65281693]\n",
            " [0.531202  ]\n",
            " [0.75240657]\n",
            " [0.43407033]\n",
            " [0.46397333]\n",
            " [0.6117117 ]\n",
            " [0.52960203]\n",
            " [0.41183422]\n",
            " [0.64711232]\n",
            " [0.46846203]\n",
            " [0.40563089]\n",
            " [0.69750581]\n",
            " [0.56298898]\n",
            " [0.46376781]\n",
            " [0.52916034]\n",
            " [0.73951687]\n",
            " [0.41919799]\n",
            " [0.49346292]\n",
            " [0.2911701 ]\n",
            " [0.49078989]\n",
            " [0.42537664]\n",
            " [0.45696538]\n",
            " [0.50965867]\n",
            " [0.32529234]\n",
            " [0.32425948]\n",
            " [0.54162103]\n",
            " [0.67096156]\n",
            " [0.37322777]\n",
            " [0.44701919]\n",
            " [0.65358166]\n",
            " [0.65407419]\n",
            " [0.57269749]\n",
            " [0.50209275]\n",
            " [0.51440029]\n",
            " [0.48304584]\n",
            " [0.53324749]\n",
            " [0.49420781]\n",
            " [0.51408811]\n",
            " [0.67620421]\n",
            " [0.65408603]\n",
            " [0.58328708]\n",
            " [0.46028206]\n",
            " [0.45277918]\n",
            " [0.39427677]\n",
            " [0.40837023]\n",
            " [0.33807249]\n",
            " [0.45103802]\n",
            " [0.59335823]\n",
            " [0.58412286]\n",
            " [0.59527833]\n",
            " [0.45945887]\n",
            " [0.36524665]\n",
            " [0.41670098]\n",
            " [0.44485692]\n",
            " [0.39666994]\n",
            " [0.37214738]\n",
            " [0.51380133]\n",
            " [0.52006045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge and lasso regularization"
      ],
      "metadata": {
        "id": "1-w4oGY3F83U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class RegularizedLinearRegression:\n",
        "    def __init__(self, input_dim, learning_rate=0.01, epochs=1000, alpha=1.0, regularization_type='ridge'):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.alpha = alpha  # Regularization strength\n",
        "        self.regularization_type = regularization_type  # 'ridge' or 'lasso'\n",
        "        self.weights = [0] * (input_dim + 1)  # +1 for the bias term\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Manual dot product and bias addition\n",
        "            linear_output = sum(x[i] * self.weights[i + 1] for i in range(len(x))) + self.weights[0]\n",
        "            predictions.append(linear_output)\n",
        "        return predictions\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m = len(y)\n",
        "        for _ in range(self.epochs):\n",
        "            predictions = self.predict(X)\n",
        "            errors = [y[i] - predictions[i] for i in range(m)]\n",
        "\n",
        "            # Compute gradients\n",
        "            gradient_weights = [0] * (len(X[0]) + 1)\n",
        "            for i in range(m):\n",
        "                for j in range(len(X[i])):\n",
        "                    gradient_weights[j + 1] -= X[i][j] * errors[i] / m\n",
        "                gradient_weights[0] -= errors[i] / m\n",
        "\n",
        "            # Add regularization term\n",
        "            if self.regularization_type == 'ridge':\n",
        "                # Ridge: L2 regularization\n",
        "                gradient_weights = [gradient_weights[i] + self.alpha * self.weights[i] for i in range(len(self.weights))]\n",
        "            elif self.regularization_type == 'lasso':\n",
        "                # Lasso: L1 regularization\n",
        "                for i in range(1, len(gradient_weights)):\n",
        "                    if self.weights[i] > 0:\n",
        "                        gradient_weights[i] += self.alpha\n",
        "                    elif self.weights[i] < 0:\n",
        "                        gradient_weights[i] -= self.alpha\n",
        "\n",
        "            # Update weights\n",
        "            self.weights = [self.weights[i] - self.learning_rate * gradient_weights[i] for i in range(len(self.weights))]\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = [[2, 3], [1, 5], [2, 1], [4, 4]]\n",
        "y = [3, 7, 4, 8]  # Continuous target values\n",
        "\n",
        "# Train Ridge Regression\n",
        "ridge = RegularizedLinearRegression(input_dim=2, learning_rate=0.01, epochs=1000, alpha=1.0, regularization_type='ridge')\n",
        "ridge.fit(X, y)\n",
        "ridge_predictions = ridge.predict(X)\n",
        "print(\"Ridge Predictions:\", ridge_predictions)\n",
        "\n",
        "# Train Lasso Regression\n",
        "lasso = RegularizedLinearRegression(input_dim=2, learning_rate=0.01, epochs=1000, alpha=1.0, regularization_type='lasso')\n",
        "lasso.fit(X, y)\n",
        "lasso_predictions = lasso.predict(X)\n",
        "print(\"Lasso Predictions:\", lasso_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPMW3l_0GApM",
        "outputId": "7a4b382b-7c9c-441e-8f35-6daadd91a630"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Predictions: [4.762276094336812, 6.004043884709963, 2.779029338522865, 7.236857403125375]\n",
            "Lasso Predictions: [5.126457872210956, 6.377133825793503, 3.6611441897563224, 6.2883901711824475]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiway classification -  softmax regression"
      ],
      "metadata": {
        "id": "kx2_JByOGihK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, input_dim, num_classes, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.num_classes = num_classes\n",
        "        self.weights = np.zeros((input_dim + 1, num_classes))  # +1 for the bias term\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        correct_log_probs = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(correct_log_probs) / m\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # Add bias term\n",
        "        scores = np.dot(X_bias, self.weights)\n",
        "        probabilities = self.softmax(scores)\n",
        "        return probabilities\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # Add bias term\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            # Forward pass\n",
        "            scores = np.dot(X_bias, self.weights)\n",
        "            probabilities = self.softmax(scores)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.cross_entropy_loss(y, probabilities)\n",
        "\n",
        "            # Backward pass\n",
        "            m = X.shape[0]\n",
        "            one_hot_y = np.zeros_like(probabilities)\n",
        "            one_hot_y[range(m), y] = 1\n",
        "\n",
        "            d_scores = (probabilities - one_hot_y) / m\n",
        "            d_weights = np.dot(X_bias.T, d_scores)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * d_weights\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if _ % 100 == 0:\n",
        "                print(f'Epoch {_}: Loss {loss}')\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = np.array([[2, 3], [1, 5], [2, 1], [4, 4]])\n",
        "y = np.array([0, 1, 0, 1])  # Binary labels for two classes (0 and 1)\n",
        "\n",
        "# Number of features and classes\n",
        "input_dim = X.shape[1]\n",
        "num_classes = 2\n",
        "\n",
        "# Train Softmax Regression\n",
        "softmax = SoftmaxRegression(input_dim=input_dim, num_classes=num_classes, learning_rate=0.01, epochs=1000)\n",
        "softmax.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "probabilities = softmax.predict(X)\n",
        "predictions = np.argmax(probabilities, axis=1)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pHJfw4SGmSr",
        "outputId": "63f2bab9-e986-4452-c2eb-60cb9d53e325"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 0.6931471805599453\n",
            "Epoch 100: Loss 0.5669266564736731\n",
            "Epoch 200: Loss 0.5268147897183293\n",
            "Epoch 300: Loss 0.5008922857365606\n",
            "Epoch 400: Loss 0.48050862507147307\n",
            "Epoch 500: Loss 0.46295849985488147\n",
            "Epoch 600: Loss 0.4472085535950009\n",
            "Epoch 700: Loss 0.43279128727862043\n",
            "Epoch 800: Loss 0.41945867547817195\n",
            "Epoch 900: Loss 0.4070574787560733\n",
            "Predictions: [1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time permit auto encoding"
      ],
      "metadata": {
        "id": "GxLRVrx1G1TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Autoencoder:\n",
        "    def __init__(self, input_dim, hidden_dim, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W_enc = np.random.randn(input_dim, hidden_dim) * 0.1\n",
        "        self.b_enc = np.zeros(hidden_dim)\n",
        "        self.W_dec = np.random.randn(hidden_dim, input_dim) * 0.1\n",
        "        self.b_dec = np.zeros(input_dim)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.encoded = self.sigmoid(np.dot(X, self.W_enc) + self.b_enc)\n",
        "        self.decoded = self.sigmoid(np.dot(self.encoded, self.W_dec) + self.b_dec)\n",
        "        return self.decoded\n",
        "\n",
        "    def fit(self, X):\n",
        "        for _ in range(self.epochs):\n",
        "            # Forward pass\n",
        "            self.forward(X)\n",
        "\n",
        "            # Compute the loss (mean squared error)\n",
        "            loss = np.mean((X - self.decoded) ** 2)\n",
        "\n",
        "            # Compute gradients for weights and biases\n",
        "            error = X - self.decoded\n",
        "            d_decoded = error * self.sigmoid_derivative(self.decoded)\n",
        "            d_encoded = np.dot(d_decoded, self.W_dec.T) * self.sigmoid_derivative(self.encoded)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.W_dec += self.learning_rate * np.dot(self.encoded.T, d_decoded) / X.shape[0]\n",
        "            self.b_dec += self.learning_rate * np.mean(d_decoded, axis=0)\n",
        "            self.W_enc += self.learning_rate * np.dot(X.T, d_encoded) / X.shape[0]\n",
        "            self.b_enc += self.learning_rate * np.mean(d_encoded, axis=0)\n",
        "\n",
        "            if _ % 100 == 0:\n",
        "                print(f'Epoch {_}: Loss {loss}')\n",
        "\n",
        "    def encode(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.W_enc) + self.b_enc)\n",
        "\n",
        "    def decode(self, encoded):\n",
        "        return self.sigmoid(np.dot(encoded, self.W_dec) + self.b_dec)\n",
        "\n",
        "# Example dataset (features)\n",
        "X = np.array([[2, 3], [1, 5], [2, 1], [4, 4]])\n",
        "\n",
        "# Create and train the autoencoder\n",
        "autoencoder = Autoencoder(input_dim=2, hidden_dim=2, learning_rate=0.01, epochs=1000)\n",
        "autoencoder.fit(X)\n",
        "\n",
        "# Encode and decode the data\n",
        "encoded = autoencoder.encode(X)\n",
        "decoded = autoencoder.decode(encoded)\n",
        "\n",
        "print(\"Original Data:\\n\", X)\n",
        "print(\"Encoded Data:\\n\", encoded)\n",
        "print(\"Decoded Data:\\n\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-RS9XIFG3Q7",
        "outputId": "ff020013-d349-4d76-aada-0619b9f19e3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 6.931339025919378\n",
            "Epoch 100: Loss 5.949756949920452\n",
            "Epoch 200: Loss 5.483716249523798\n",
            "Epoch 300: Loss 5.304285697080962\n",
            "Epoch 400: Loss 5.217841787659971\n",
            "Epoch 500: Loss 5.168358332611501\n",
            "Epoch 600: Loss 5.136675474391282\n",
            "Epoch 700: Loss 5.114783014511559\n",
            "Epoch 800: Loss 5.098805362778081\n",
            "Epoch 900: Loss 5.086657096256903\n",
            "Original Data:\n",
            " [[2 3]\n",
            " [1 5]\n",
            " [2 1]\n",
            " [4 4]]\n",
            "Encoded Data:\n",
            " [[0.9450075  0.93305885]\n",
            " [0.9658088  0.96937958]\n",
            " [0.85709123 0.81155749]\n",
            " [0.98881173 0.98073883]]\n",
            "Decoded Data:\n",
            " [[0.9686813  0.98210731]\n",
            " [0.97054643 0.98335729]\n",
            " [0.9607166  0.97668075]\n",
            " [0.97168271 0.98407116]]\n"
          ]
        }
      ]
    }
  ]
}